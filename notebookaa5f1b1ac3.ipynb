{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ninput_tensor = torch.tensor([\n    [1.0, 0.5, 0.3],\n    [0.2, 0.1, 0.7],\n    [0.4, 0.8, 0.6]\n])  \n\nW_Q = torch.rand(3, 3)\nW_K = torch.rand(3, 3)\nW_V = torch.rand(3, 3)\n\nQ = input_tensor @ W_Q \nK = input_tensor @ W_K\nV = input_tensor @ W_V\n\nd_k = K.shape[-1]\nattention_scores = Q @ K.T / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n\nattention_weights = F.softmax(attention_scores, dim=-1)\n\noutput = attention_weights @ V\n\nprint(\"Attention Scores:\\n\", attention_scores)\nprint(\"Attention Weights (after softmax):\\n\", attention_weights)\nprint(\"Output after Self-Attention:\\n\", output)\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-29T21:58:19.759586Z","iopub.execute_input":"2025-03-29T21:58:19.760002Z","iopub.status.idle":"2025-03-29T21:58:24.341204Z","shell.execute_reply.started":"2025-03-29T21:58:19.759959Z","shell.execute_reply":"2025-03-29T21:58:24.339771Z"}},"outputs":[{"name":"stdout","text":"Attention Scores:\n tensor([[1.4079, 0.5705, 1.3870],\n        [0.7807, 0.3727, 0.7872],\n        [1.2414, 0.5781, 1.2422]])\nAttention Weights (after softmax):\n tensor([[0.4146, 0.1794, 0.4060],\n        [0.3743, 0.2489, 0.3768],\n        [0.3975, 0.2048, 0.3978]])\nOutput after Self-Attention:\n tensor([[1.1806, 0.7543, 0.6729],\n        [1.1221, 0.7147, 0.6509],\n        [1.1588, 0.7401, 0.6656]])\n","output_type":"stream"}],"execution_count":1}]}